# Single Instance: Training + MLflow Together
# This runs on your on-demand training instance

version: "3.8"

services:
  # MLflow Tracking Server (local to training instance)
  mlflow:
    image: python:3.10-slim
    container_name: mlflow-training
    ports:
      - "5000:5000"
    environment:
      # Backend store: PostgreSQL for metadata
      BACKEND_STORE_URI: postgresql://mlflow:${POSTGRES_PASSWORD}@postgres:5432/mlflowdb
      # Artifact store: S3 for model files (persists across shutdowns)
      DEFAULT_ARTIFACT_ROOT: s3://ai-drug-data/mlflow-artifacts
      # AWS credentials (use IAM role - already configured on your instance)
      AWS_DEFAULT_REGION: eu-north-1
    command: >
      bash -c "
        pip install mlflow psycopg2-binary boto3 &&
        mlflow server 
          --host 0.0.0.0 
          --port 5000 
          --backend-store-uri postgresql://mlflow:${POSTGRES_PASSWORD}@postgres:5432/mlflowdb
          --default-artifact-root s3://ai-drug-data/mlflow-artifacts
          --serve-artifacts
      "
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - training-network

  # PostgreSQL Database (data persists on EBS volume)
  postgres:
    image: postgres:15-alpine
    container_name: mlflow-postgres-training
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: mlflowdb
    volumes:
      # Mount to EBS volume that persists across instance restarts
      - /mnt/mlflow-data/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow -d mlflowdb"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - training-network

  # Training Environment (your existing training pipeline)
  training:
    image: ${TRAINING_IMAGE:-ghcr.io/j0mt/ai-drug-training:latest}
    container_name: ai-drug-training
    environment:
      # Point to local MLflow server
      MLFLOW_TRACKING_URI: http://mlflow:5000
      # DVC and training environment variables
      DVC_REMOTE: s3://ai-drug-data
      PYTHONPATH: /app
      # AWS credentials for DVC S3 access
      AWS_DEFAULT_REGION: eu-north-1
    volumes:
      # Mount data directory only (code is in the image)
      - /mnt/training-data:/data
      # Mount AWS credentials if needed
      - ~/.aws:/root/.aws:ro
    depends_on:
      mlflow:
        condition: service_started
    working_dir: /app
    # Training runs on-demand, not as a service
    profiles:
      - training
    networks:
      - training-network

networks:
  training-network:
    driver: bridge